# model_integration_example.py
"""
Example demonstrating how to integrate the ModelManager with a Streamlit chat application.
This shows how to use the model management system with tier selection and streaming responses.
"""

import streamlit as st
from typing import List, Dict
import time

# Import our model management components
from model_manager import ModelManager, ModelTier, get_model_manager, get_session_model_tier
from model_ui import (
    render_model_selector, 
    render_model_status_indicator,
    render_model_settings_sidebar,
    render_model_comparison
)

def initialize_session_state():
    """Initialize session state variables"""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "model_manager" not in st.session_state:
        st.session_state.model_manager = get_model_manager()

def handle_model_change(new_tier: ModelTier):
    """Handle model tier change"""
    st.success(f"Switched to {new_tier.value} mode!")
    st.rerun()

def simulate_chat_response(messages: List[Dict], model_manager: ModelManager) -> str:
    """
    Simulate a chat response using the model manager.
    In a real application, this would integrate with your RAG pipeline.
    """
    try:
        # Get the current model configuration
        current_config = model_manager.get_current_model()
        
        # Simulate different response times based on model tier
        if current_config.tier == ModelTier.FAST:
            time.sleep(0.5)  # Fast model - quick response
        else:
            time.sleep(1.0)  # Premium model - slower but higher quality
        
        # For demo purposes, return a mock response
        # In real implementation, use: model_manager.generate_response(messages)
        return f"This is a response from {current_config.display_name}. " \
               f"The model is optimized for {current_config.description.lower()}."
    
    except Exception as e:
        st.error(f"Error generating response: {str(e)}")
        return "I apologize, but I encountered an error generating a response."

def simulate_streaming_response(messages: List[Dict], model_manager: ModelManager):
    """
    Simulate a streaming chat response using the model manager.
    """
    try:
        current_config = model_manager.get_current_model()
        
        # Mock streaming response
        response_text = f"This is a streaming response from {current_config.display_name}. " \
                       f"Each chunk arrives progressively to create a typing effect. " \
                       f"The {current_config.tier.value} model provides " \
                       f"{'quick' if current_config.tier == ModelTier.FAST else 'detailed'} responses."
        
        # Simulate streaming by yielding chunks
        words = response_text.split()
        for i in range(0, len(words), 3):  # Yield 3 words at a time
            chunk = " ".join(words[i:i+3]) + " "
            yield chunk
            time.sleep(0.1)  # Simulate network delay
    
    except Exception as e:
        yield f"Error: {str(e)}"

def main():
    """Main application function"""
    st.set_page_config(
        page_title="Model Management Demo",
        page_icon="ü§ñ",
        layout="wide"
    )
    
    # Initialize session state
    initialize_session_state()
    model_manager = st.session_state.model_manager
    
    # Sidebar with model settings
    render_model_settings_sidebar(model_manager)
    
    # Main content
    st.title("ü§ñ Model Management System Demo")
    st.markdown("This demo shows how to integrate the ModelManager with a chat interface.")
    
    # Model selection section
    st.markdown("## Model Selection")
    current_tier = render_model_selector(
        model_manager=model_manager,
        on_change=handle_model_change,
        key="main_selector"
    )
    
    # Current model status
    st.markdown("## Current Model Status")
    render_model_status_indicator(model_manager)
    
    # Model comparison
    with st.expander("üìä Model Comparison"):
        render_model_comparison()
    
    # Chat interface demo
    st.markdown("## Chat Interface Demo")
    
    # Display chat messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if "model_info" in message:
                st.caption(f"Generated by: {message['model_info']}")
    
    # Chat input
    if prompt := st.chat_input("Ask a pharmacology question..."):
        # Add user message
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Display user message
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Generate and display assistant response
        with st.chat_message("assistant"):
            # Show which model is being used
            current_config = model_manager.get_current_model()
            st.caption(f"Using: {current_config.display_name}")
            
            # Option 1: Non-streaming response
            if st.checkbox("Use streaming response", value=True):
                # Streaming response
                response_placeholder = st.empty()
                full_response = ""
                
                for chunk in simulate_streaming_response(st.session_state.messages, model_manager):
                    full_response += chunk
                    response_placeholder.markdown(full_response + "‚ñå")
                
                response_placeholder.markdown(full_response)
            else:
                # Non-streaming response
                with st.spinner("Generating response..."):
                    response = simulate_chat_response(st.session_state.messages, model_manager)
                st.markdown(response)
                full_response = response
            
            # Add assistant message to history
            st.session_state.messages.append({
                "role": "assistant", 
                "content": full_response,
                "model_info": current_config.display_name
            })
    
    # Clear chat button
    if st.button("Clear Chat History"):
        st.session_state.messages = []
        st.rerun()
    
    # Debug information
    with st.expander("üîß Debug Information"):
        st.json({
            "current_model_tier": current_tier.value,
            "model_config": model_manager.get_model_info(current_tier),
            "session_messages": len(st.session_state.messages),
            "api_key_configured": bool(model_manager.api_key)
        })

def test_model_functionality():
    """Test page for model functionality"""
    st.title("üß™ Model Functionality Tests")
    
    try:
        model_manager = get_model_manager()
        st.success("‚úÖ ModelManager initialized successfully")
        
        # Test model configurations
        st.subheader("Model Configurations")
        for tier in [ModelTier.FAST, ModelTier.PREMIUM]:
            config = model_manager.get_model_config(tier)
            st.write(f"**{config.display_name}**")
            st.json(model_manager.get_model_info(tier))
        
        # Test model switching
        st.subheader("Model Switching Test")
        if st.button("Switch to Fast Mode"):
            model_manager.set_current_model(ModelTier.FAST)
            st.success("Switched to Fast Mode")
        
        if st.button("Switch to Premium Mode"):
            model_manager.set_current_model(ModelTier.PREMIUM)
            st.success("Switched to Premium Mode")
        
        current_model = model_manager.get_current_model()
        st.info(f"Current Model: {current_model.display_name}")
        
    except Exception as e:
        st.error(f"‚ùå Error testing model functionality: {str(e)}")
        st.exception(e)

if __name__ == "__main__":
    # Check if we're running the test page
    if st.sidebar.button("Show Test Page"):
        test_model_functionality()
    else:
        main()